{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from tqdm.auto import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))])\n",
    "\n",
    "# # Download CiFAR-100 Dataset from PyTorch\n",
    "# training_data = datasets.CIFAR100(\n",
    "#                 root=\"data\",                        # Set root directory of data\n",
    "#                 train=True,                         # Get training dataset\n",
    "#                 download=True,                      # Download the data\n",
    "#                 transform=transform)                # Transform the dataset into tensors and normalise\n",
    "\n",
    "# testing_data = datasets.CIFAR100(\n",
    "#                 root=\"data\",                        # Set root directory of data\n",
    "#                 train=False,                        # Get testing dataset\n",
    "#                 download=True,                      # Download the data\n",
    "#                 transform=transform)                # Transform the dataset into tensors and normalise        \n",
    "\n",
    "# print(f\"Length of training data: {len(training_data)}\")\n",
    "# print(f\"Length of testing data: {len(testing_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Length of training data: 50000\n",
      "Length of testing data: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616))])\n",
    "\n",
    "# Download CiFAR-10 Dataset from PyTorch\n",
    "training_data = datasets.CIFAR10(\n",
    "                root=\"data\",                        # Set root directory of data\n",
    "                train=True,                         # Get training dataset\n",
    "                download=True,                      # Download the data\n",
    "                transform=transform)                # Transform the dataset into tensors and normalise\n",
    "\n",
    "testing_data = datasets.CIFAR10(\n",
    "                root=\"data\",                        # Set root directory of data\n",
    "                train=False,                        # Get testing dataset\n",
    "                download=True,                      # Download the data\n",
    "                transform=transform)                # Transform the dataset into tensors and normalise        \n",
    "\n",
    "print(f\"Length of training data: {len(training_data)}\")\n",
    "print(f\"Length of testing data: {len(testing_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of an image ([colour channels, height, width])\n",
    "# image, label = training_data[0]\n",
    "# print(f\"Image Shape: {image.shape}\")\n",
    "# print(f\"Label: {training_data.classes[label]}\")\n",
    "\n",
    "# plt.figure(figsize=(3,3))\n",
    "# plt.imshow(transforms.ToPILImage()(image))\n",
    "# plt.title(training_data.classes[label])\n",
    "# plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(training_data.classes)\n",
    "print(NUM_CLASSES)\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [40000, 10000])\n",
    "\n",
    "# Put training and testing data in dataloaders for efficient training\n",
    "training_dataloader = DataLoader(dataset=training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "testing_dataloader = DataLoader(dataset=testing_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VGG-11 architecture\n",
    "# 11 Weight layers -> Flatten -> 3 FC Layers -> Softmax\n",
    "# VGG_11 = 64, MaxPool, 128, MaxPool, 256, 256, MaxPool, 512, 512, MaxPool, 512, 512, MaxPool\n",
    "\n",
    "class VGG_11(nn.Module):\n",
    "    def __init__(self, in_features = 3, num_classes = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        # Every conv2d layer has a kernel size 3, stride 1 and padding 1\n",
    "        # All 5 max pool layers has kernel size 2 and stride 2\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_features, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            ) \n",
    "  \n",
    "        # After the last max pool layer, VGG has 3 fully connected layer\n",
    "        # in_features = last_out_channels*(img_height/stride**num_max_pool)*(img_width/stride**num_max_pool)\n",
    "        # 512*(32/2**5)*(32/2**5)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=512*1*1, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VGG-13 architecture\n",
    "# 13 Weight layers -> Flatten -> 3 FC Layers -> Softmax\n",
    "# VGG_13 = 64, 64, MaxPool, 128, 128, MaxPool, 256, 256, MaxPool, 512, 512, MaxPool, 512, 512, MaxPool\n",
    "\n",
    "class VGG_13(nn.Module):\n",
    "    def __init__(self, in_features = 3, num_classes = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        # Every conv2d layer has a kernel size 3, stride 1 and padding 1\n",
    "        # All 5 max pool layers has kernel size 2 and stride 2\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_features, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            ) \n",
    "  \n",
    "        # After the last max pool layer, VGG has 3 fully connected layer\n",
    "        # in_features = last_out_channels*(img_height/stride**num_max_pool)*(img_width/stride**num_max_pool)\n",
    "        # 512*(32/2**5)*(32/2**5)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=512*1*1, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VGG-16 architecture\n",
    "# 16 Weight layers -> Flatten -> 3 FC Layers -> Softmax\n",
    "# VGG_16 = 64, 64, MaxPool, 128, 128, MaxPool, 256, 256, 256, MaxPool, 512, 512, 512, MaxPool, 512, 512, 512, MaxPool\n",
    "\n",
    "class VGG_16(nn.Module):\n",
    "    def __init__(self, in_features = 3, num_classes = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        # Every conv2d layer has a kernel size 3, stride 1 and padding 1\n",
    "        # All 5 max pool layers has kernel size 2 and stride 2\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_features, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            ) \n",
    "  \n",
    "        # After the last max pool layer, VGG has 3 fully connected layer\n",
    "        # in_features = last_out_channels*(img_height/stride**num_max_pool)*(img_width/stride**num_max_pool)\n",
    "        # 512*(32/2**5)*(32/2**5)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=512*1*1, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print out layer and number of parameters in a table\n",
    "def model_summary(model):\n",
    "    summary = PrettyTable([\"Layers\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for layer, parameter in model.named_parameters():\n",
    "        # Skip non-trainable parameters\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        # Returns num of params in tensor\n",
    "        params = parameter.numel()\n",
    "        summary.add_row([layer, params])\n",
    "        total_params += params\n",
    "    print(summary)\n",
    "    print(f\"Total Params: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the time difference between training start time and training end time\n",
    "def print_train_time(start: torch.float, end: torch.float):\n",
    "    total = end - start\n",
    "    print(f\"Training time: {total:.3f} seconds\")\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(iter(training_dataloader))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted,actual):\n",
    "    _, predictions = torch.max(predicted, dim=1)\n",
    "    return torch.tensor(torch.sum(predictions==actual).item()/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------+\n",
      "|         Layers        | Parameters |\n",
      "+-----------------------+------------+\n",
      "|  conv_layers.0.weight |    1728    |\n",
      "|   conv_layers.0.bias  |     64     |\n",
      "|  conv_layers.2.weight |   36864    |\n",
      "|   conv_layers.2.bias  |     64     |\n",
      "|  conv_layers.5.weight |   73728    |\n",
      "|   conv_layers.5.bias  |    128     |\n",
      "|  conv_layers.7.weight |   147456   |\n",
      "|   conv_layers.7.bias  |    128     |\n",
      "| conv_layers.10.weight |   294912   |\n",
      "|  conv_layers.10.bias  |    256     |\n",
      "| conv_layers.12.weight |   589824   |\n",
      "|  conv_layers.12.bias  |    256     |\n",
      "| conv_layers.15.weight |  1179648   |\n",
      "|  conv_layers.15.bias  |    512     |\n",
      "| conv_layers.17.weight |  2359296   |\n",
      "|  conv_layers.17.bias  |    512     |\n",
      "| conv_layers.20.weight |  2359296   |\n",
      "|  conv_layers.20.bias  |    512     |\n",
      "| conv_layers.22.weight |  2359296   |\n",
      "|  conv_layers.22.bias  |    512     |\n",
      "|      fc.0.weight      |  2097152   |\n",
      "|       fc.0.bias       |    4096    |\n",
      "|      fc.3.weight      |  16777216  |\n",
      "|       fc.3.bias       |    4096    |\n",
      "|      fc.6.weight      |   40960    |\n",
      "|       fc.6.bias       |     10     |\n",
      "+-----------------------+------------+\n",
      "Total Params: 28328522\n"
     ]
    }
   ],
   "source": [
    "# model = VGG_11(in_features=3, num_classes=NUM_CLASSES).to(device=device)\n",
    "model = VGG_13(in_features=3, num_classes=NUM_CLASSES).to(device=device)\n",
    "# model = VGG_16(in_features=3, num_classes=NUM_CLASSES).to(device=device)\n",
    "model_summary(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optim = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# train_time_start = time.time()\n",
    "# for epoch in tqdm(range(EPOCHS)):\n",
    "#     training_loss = 0\n",
    "\n",
    "#     # Enumerate over all batches\n",
    "#     for image, label in training_dataloader:\n",
    "#         image, label = image.to(device), label.to(device)\n",
    "#         model.train()\n",
    "\n",
    "#         # Pass training batch into model for prediction\n",
    "#         pred = model(image)\n",
    "#         loss = loss_fn(pred, label)\n",
    "#         training_loss += loss\n",
    "        \n",
    "#         optim.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "\n",
    "#     # Calculate average training loss\n",
    "#     training_loss /= len(training_dataloader)\n",
    "\n",
    "#     validation_loss, validation_acc = 0, 0\n",
    "\n",
    "#     # Disable weight updating\n",
    "#     model.eval()\n",
    "#     with torch.inference_mode():\n",
    "#         for image, label in validation_dataloader:\n",
    "#             image, label = image.to(device), label.to(device)\n",
    "\n",
    "#             pred = model(image)\n",
    "#             loss = loss_fn(pred, label)\n",
    "#             validation_loss += loss\n",
    "            \n",
    "#             validation_acc += accuracy(pred, label)\n",
    "\n",
    "#         validation_loss /= len(validation_dataloader)\n",
    "#         validation_acc /= len(validation_dataloader)\n",
    "    \n",
    "#     if (epoch % 10) == 0:\n",
    "#         print(f\"Training Loss: {training_loss:.5f} | Validation Loss: {validation_loss:.5f} | Validation Accuracy: {validation_acc:.5f}\")\n",
    "\n",
    "# train_time_end = time.time()\n",
    "# total_time = print_train_time(start=train_time_start, end=train_time_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:21<17:41, 21.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.98167 | Validation Loss: 1.75815 | Validation Accuracy: 0.29912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [03:46<13:15, 20.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.39555 | Validation Loss: 0.86869 | Validation Accuracy: 0.74385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [07:10<09:51, 20.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.12935 | Validation Loss: 1.01095 | Validation Accuracy: 0.75244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [10:33<06:27, 20.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.07469 | Validation Loss: 1.28119 | Validation Accuracy: 0.76709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [13:57<03:03, 20.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.07608 | Validation Loss: 1.59647 | Validation Accuracy: 0.76650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [17:01<00:00, 20.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1021.328 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optim = torch.optim.SGD(params=model.parameters(), lr=0.1)\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_time_start = time.time()\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    training_loss = 0\n",
    "\n",
    "    # Enumerate over all batches\n",
    "    for image, label in training_dataloader:\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        model.train()\n",
    "\n",
    "        # Pass training batch into model for prediction\n",
    "        pred = model(image)\n",
    "        loss = loss_fn(pred, label)\n",
    "        training_loss += loss\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    # Calculate average training loss\n",
    "    training_loss /= len(training_dataloader)\n",
    "\n",
    "    validation_loss, validation_acc = 0, 0\n",
    "\n",
    "    # Disable weight updating\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for image, label in validation_dataloader:\n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            pred = model(image)\n",
    "            loss = loss_fn(pred, label)\n",
    "            validation_loss += loss\n",
    "            \n",
    "            validation_acc += accuracy(pred, label)\n",
    "\n",
    "        validation_loss /= len(validation_dataloader)\n",
    "        validation_acc /= len(validation_dataloader)\n",
    "    \n",
    "    if (epoch % 10) == 0:\n",
    "        print(f\"Training Loss: {training_loss:.5f} | Validation Loss: {validation_loss:.5f} | Validation Accuracy: {validation_acc:.5f}\")\n",
    "\n",
    "train_time_end = time.time()\n",
    "total_time = print_train_time(start=train_time_start, end=train_time_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss: 1.8873 | Testing Accuracy: 0.7559 \n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = 0, 0\n",
    "model.eval()\n",
    "\n",
    "testing_loss, testing_acc = 0, 0\n",
    "\n",
    "# Disable weight updating\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for image, label in testing_dataloader:\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        pred = model(image)\n",
    "        loss = loss_fn(pred, label)\n",
    "        testing_loss += loss\n",
    "        \n",
    "        testing_acc += accuracy(pred, label)\n",
    "\n",
    "    testing_loss /= len(testing_dataloader)\n",
    "    testing_acc /= len(testing_dataloader)\n",
    "    print(f\"Testing Loss: {testing_loss:.4f} | Testing Accuracy: {testing_acc:.4f} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
